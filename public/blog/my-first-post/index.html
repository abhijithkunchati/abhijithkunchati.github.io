<!DOCTYPE html>
<html>
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
	<title>Abhijith Kunchati</title>
	<meta charset='UTF-8'>
	<meta content='width=device-width, initial-scale=1' name='viewport'/>

	<meta name='description' content="Abhijith Kunchati is a software engineer.">

    <meta name='keywords' 
		content="[computer science distributed systems software engineering backend development full stack development]">
	<meta name='author' content='Abhijith Kunchati'>

	<link rel='stylesheet' type='text/css' href='/styles/main.css'/>
    <link rel='stylesheet' type='text/css' href='/styles/blog.css'/>
	<link rel='shortcut icon' href='/favicon.png?v=e' />
</head>
<body>
    
<div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
    </ul>
</div>




<div class='content'>
    <div class='front-matter'>
        <div class='wrap'>
            <h1>My First Post</h1>
            <h4>this is summary of first post</h4>
            <div class='bylines'>
                <div class='byline'>
                    <h3>Published</h3>
                    <p>January 27, 2025</p>
                </div>
            </div>
            <div class='clear'></div>
        </div>
    </div>
    <div class='wrap article'>
        <p>Finally, we need to use a property of the continuous-time process. If Z is a lognormal random variable with parameters ν and s2, then its expected value is</p>
<p>Large language models (LLMs) still feel a bit like magic to me. Of course, I understand the general machinery enough to <strong>know that they aren’t</strong>, but the gap between my outdated knowledge of the field and the state-of-the-art feels <em>especially large right now</em>. Things are moving fast. So six months ago, I decided to close that gap just a little by digging into what I believed was one of the core primitives underpinning LLMs: the attention mechanism in neural networks.</p>
<blockquote>
<p>I started by reading one of the landmark papers in the literature, which was published by Google Brain in 2017 under the catchy title Attention is all you need (Vaswani et al., 2017). As the title suggests, the authors did not invent the attention mechanism. Rather, they introduced a neural network architecture which in was some sense “all attention”. This architecture is the now-famous transformer. Clearly the transformer stands in contrast to whatever came before it, but what was that and what did the transformer do differently?</p>
</blockquote>
<p>To answer these questions, I read a lot of papers, and the context that felt natural to provide here grew the more that I read  &lt;  I went down the rabbit hole, and when I came out, I realized that what had started as a study of attention had grown into a bigger story.<aside> Attention is still the throughline,especially large right now. Things are moving fast. So six months ago, I decided to close that gap just a little by digging into what I believed was one of the core primitives underpinning LLMs: the attention mechanism in neural</aside><aside class='rightnote'>but there are other important themes, such as how neural networks generalize and the bitter lesson that simple methods that scale seem to triumph over clever methods which do not. </aside>This post is the product of that deep dive, and it is a stylized history of LLMs.</p>
<p>Large language models (LLMs) still feel a bit like magic to me. Of course, I understand the general machinery enough to know that they aren’t, but the gap between my outdated knowledge of the field and the state-of-the-art feels especially large right now. Things are moving fast. So six months ago, I decided to close that gap just a little by digging into what I believed was one of the core primitives underpinning LLMs: the attention mechanism in neural networks.</p>
<p>I started by reading one of the landmark papers in the literature, which was published by Google Brain in 2017 under the catchy title Attention is all you need (Vaswani et al., 2017). As the title suggests, the authors did not invent the attention mechanism. Rather, they introduced a neural network architecture which in was some sense “all attention”. This architecture is the now-famous transformer. Clearly the transformer stands in contrast to whatever came before it, but what was that and what did the transformer do differently?</p>
<p>To answer these questions, I read a lot of papers, and the context that felt natural to provide here grew the more that I read. <aside class='rightnote'>I went down the rabbit hole, and when I came out, I realized that what had started as a study of attention had grown into a bigger story. Attention is still the throughline, but there are </aside>other important themes, such as how neural networks generalize and the bitter lesson that simple methods that scale seem to triumph over clever methods which do not. This post is the product of that deep dive, and it is a stylized history of LLMs.
Large language models (LLMs) still feel a bit like magic to me. Of course, I understand the general machinery enough to know that they aren’t, but the gap between my outdated knowledge of the field and the state-of-the-art feels especially large right now. Things are moving fast. So six months ago, I decided to close that gap just a little by digging into what I believed was one of the core primitives underpinning LLMs: the attention mechanism in neural networks.</p>
<p>I started by reading one of the landmark papers in the literature, which was published by Google Brain in 2017 under the catchy title Attention is all you need (Vaswani et al., 2017). As the title suggests, the authors did not invent the attention mechanism. Rather, they introduced a neural network architecture which in was some sense “all attention”. This architecture is the now-famous transformer. Clearly the transformer stands in contrast to whatever came before it, but what was that and what did the transformer do differently?</p>
<hr>
<p>To answer these questions, I read a lot of papers, and the context that felt natural to provide here grew the more that I read. I went down the rabbit hole, and when I came out, I realized that what had started as a study of attention had grown into a bigger story. Attention is still the throughline, but there are other important themes, such as how neural networks generalize and the bitter lesson that simple methods that scale seem to triumph over clever methods which do not. This post is the product of that deep dive, and it is a stylized history of LLMs.
Large language models (LLMs) still feel a bit like magic to me. Of course, I understand the general machinery enough to know that they aren’t, but the gap between my outdated knowledge of the field and the state-of-the-art feels especially large right now. Things are moving fast. So six months ago, I decided to close that gap just a little by digging into what I believed was one of the core primitives underpinning LLMs: the attention mechanism in neural networks.</p>
<h1 id="header-1">Header 1</h1>
<p>I started by reading one of the landmark papers in the literature, which was published by Google Brain in 2017 under the catchy title Attention is all you need (Vaswani et al., 2017). As the title suggests, the authors did not invent the attention mechanism. Rather, <aside class='rightnote'> they introduced a neural network architecture which in was some sense “all attention”.</aside>This architecture is the now-famous transformer. Clearly the transformer stands in contrast to whatever came before it, but what was that and what did the transformer do differently?</p>
<h2 id="header-2">Header 2</h2>
<p>To answer these questions, I read a lot of papers, and the context that felt natural to provide here grew the more that I read. I went down the rabbit hole, and when I came out, I realized that what had started as a study of attention had grown into a bigger story. Attention is still the throughline, but there are other important themes, such as how neural networks generalize and the bitter lesson that simple methods that scale seem to triumph over clever methods which do not. This post is the product of that deep dive, and it is a stylized history of LLMs.</p>
<h2 id="header-3">Header 3</h2>
<p>To answer these questions, I read a lot of papers, and the context that felt natural to provide here grew the more that I read. I went down the rabbit hole, and when I came out, I realized that what had started as a study of attention had grown into a bigger story. Attention is still the throughline, but there are other important themes, such as how neural networks generalize and the bitter lesson that simple methods that scale seem to triumph over clever methods which do not. This post is the product of that deep dive, and it is a stylized history of LLMs.</p>

    </div>
</div>



</body>
</html>
